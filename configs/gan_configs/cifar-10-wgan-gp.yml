gan_config:
  dp: true
  dataset: 
    name: cifar10
    params: {}
  train_transform:
    Normalize:
      mean: &mean [0.5, 0.5, 0.5]
      std: &std [0.5, 0.5, 0.5]
  prior: uniform
  generator:
    name: SN_DCGAN_Generator
    params:
      mean: *mean
      std: *std
    ckpt_path: checkpoints/wgan_gp_dot/DCGAN_G_CIFAR_WGAN-GP_150001.pth
  discriminator:
    name: WGANDiscriminator
    params:
      mean: *mean
      std: *std
    ckpt_path: checkpoints/wgan_gp_dot/DCGAN_D_CIFAR_WGAN-GP_150001.pth
  thermalize:
    true: {}
    false:
      log_norm_const: 3.1634721852656957
      lipschitz_const: 10.185486975097659
      fake_score: &fake_score -1.736105637550354
      real_score: &real_score 2.3100808167457583
      mean_score: &mean_score 0.2869875895977021

train_batch_size: &train_batch_size 64 #256 #128
n_epochs: &n_epochs 200 #150 #0
criterion_g: &criterion_g
  name: Wasserstein1Loss
criterion_d: &criterion_d
  name: Wasserstein1Loss
optimizer_g: &optimizer_g
  name: Adam
  params:
    lr: &g_lr 0.0001
    betas: [0.5, 0.9]
optimizer_d: &optimizer_d
  name: Adam
  params:
    lr: 0.0001
    betas: [0.5, 0.9]
trainer_kwargs: &trainer_kwargs
  n_dis: 3
  sample_size: 10000 #512 #5000
  grad_acc_steps: 1
  sample_steps: &sample_steps 1
  alpha: 300
  gp_coef: 10.0 #1000.0


