{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "from collections import Mapping\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "import ruamel.yaml as yaml\n",
    "\n",
    "\n",
    "# sys.path.append('../studiogan')\n",
    "\n",
    "ROOT_DIR = Path('..')\n",
    "CONFIGS_DIR = Path(ROOT_DIR, \"configs\")\n",
    "\n",
    "\n",
    "class DotConfig(Mapping):\n",
    "    \"\"\"\n",
    "    Simple wrapper for config\n",
    "    allowing access with dot notation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, yaml):\n",
    "        self._dict = yaml\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        if key in self.__dict__:\n",
    "            return super().__getattr__(key)\n",
    "        if key in self._dict:\n",
    "            value = self._dict[key]\n",
    "            if isinstance(value, dict):\n",
    "                return DotConfig(value)\n",
    "            return value\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def items(self):\n",
    "        return [(k, DotConfig(v)) for k, v in self._dict.items()]\n",
    "\n",
    "    def keys(self):\n",
    "        return self._dict.keys()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dict)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._dict.__iter__()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._dict[key]\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self._dict\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self._dict\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self._dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ModelRegistry:\n",
    "    registry = {}\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, name: Optional[str] = None) -> nn.Module:\n",
    "        def inner_wrapper(wrapped_class: nn.Module) -> nn.Module:\n",
    "            if name is None:\n",
    "                name_ = wrapped_class.__name__\n",
    "            else:\n",
    "                name_ = name\n",
    "            cls.registry[name_] = wrapped_class\n",
    "            return wrapped_class\n",
    "\n",
    "        return inner_wrapper\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, name: str, **kwargs) -> nn.Module:\n",
    "        model = cls.registry[name]\n",
    "        model = model(**kwargs)\n",
    "        return model\n",
    "\n",
    "\n",
    "class NormalizeInverse(transforms.Normalize):\n",
    "    \"\"\"\n",
    "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-10)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())\n",
    "\n",
    "\n",
    "class BaseDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mean: Tuple[float, float, float],\n",
    "        std: Tuple[float, float, float],\n",
    "        output_layer: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transform = transforms.Normalize(mean, std)\n",
    "        if output_layer == \"sigmoid\":\n",
    "            self.output_layer = nn.Sigmoid()\n",
    "        else:\n",
    "            self.output_layer = nn.Identity()\n",
    "\n",
    "    def get_label(self) -> torch.LongTensor:\n",
    "        return self.label.data.long()\n",
    "\n",
    "\n",
    "class BaseGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, mean: Tuple[float, float, float], std: Tuple[float, float, float]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.inverse_transform = transforms.Compose(\n",
    "            [\n",
    "                NormalizeInverse(mean, std),\n",
    "                transforms.Lambda(lambda x: torch.clip(x, 0, 1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def sample_label(self, *args, **kwargs):\n",
    "        return None\n",
    "\n",
    "    def get_label(self) -> torch.LongTensor:\n",
    "        return self.label.data.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "\n",
    "def _upsample(x):\n",
    "    h, w = x.shape[2:]\n",
    "    return F.interpolate(x, size=(h * 2, w * 2), mode=\"nearest\")\n",
    "\n",
    "\n",
    "def upsample_conv(x, conv):\n",
    "    return conv(_upsample(x))\n",
    "\n",
    "\n",
    "@ModelRegistry.register()\n",
    "class SN_DCGAN_Generator(BaseGenerator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden=128,\n",
    "        bw=4,\n",
    "        ch=512,\n",
    "        nz=128,\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5),\n",
    "    ):\n",
    "        super().__init__(mean, std)\n",
    "        self.z_dim = nz\n",
    "        self.ch = ch\n",
    "        self.bw = bw\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.l0 = nn.Linear(n_hidden, bw * bw * ch)\n",
    "        self.dc1 = nn.ConvTranspose2d(ch, ch // 2, 4, 2, 1)\n",
    "        self.dc2 = nn.ConvTranspose2d(ch // 2, ch // 4, 4, 2, 1)\n",
    "        self.dc3 = nn.ConvTranspose2d(ch // 4, ch // 8, 4, 2, 1)\n",
    "        self.dc4 = nn.ConvTranspose2d(ch // 8, 3, 3, 1, 1)\n",
    "        self.bn0 = nn.BatchNorm2d(bw * bw * ch, eps=2e-5, momentum=0.1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch // 2, eps=2e-5, momentum=0.1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch // 4, eps=2e-5, momentum=0.1)\n",
    "        self.bn3 = nn.BatchNorm2d(ch // 8, eps=2e-5, momentum=0.1)\n",
    "\n",
    "    def forward(self, z, **kwargs):\n",
    "        h = self.l0(z)\n",
    "        h = h.view(-1, self.ch * self.bw * self.bw, 1, 1)\n",
    "        h = self.relu(self.bn0(h))\n",
    "        h = h.view(-1, self.ch, self.bw, self.bw)\n",
    "        h = self.relu(self.bn1(self.dc1(h)))\n",
    "        h = self.relu(self.bn2(self.dc2(h)))\n",
    "        h = self.relu(self.bn3(self.dc3(h)))\n",
    "        o = self.tanh(self.dc4(h))\n",
    "        return o\n",
    "\n",
    "\n",
    "@ModelRegistry.register()\n",
    "class SN_DCGAN_Discriminator(BaseDiscriminator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bw=4,\n",
    "        ch=512,\n",
    "        output_dim=1,\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5),\n",
    "        output_layer=\"identity\",\n",
    "    ):\n",
    "        super().__init__(mean, std, output_layer)\n",
    "        c0_0 = nn.Conv2d(3, ch // 8, 3, 1, 1)\n",
    "        self.c0_0 = spectral_norm(c0_0)\n",
    "\n",
    "        c0_1 = nn.Conv2d(ch // 8, ch // 4, 4, 2, 1)\n",
    "        self.c0_1 = spectral_norm(c0_1)\n",
    "\n",
    "        c1_0 = nn.Conv2d(ch // 4, ch // 4, 3, 1, 1)\n",
    "        self.c1_0 = spectral_norm(c1_0)\n",
    "\n",
    "        c1_1 = nn.Conv2d(ch // 4, ch // 2, 4, 2, 1)\n",
    "        self.c1_1 = spectral_norm(c1_1)\n",
    "\n",
    "        c2_0 = nn.Conv2d(ch // 2, ch // 2, 3, 1, 1)\n",
    "        self.c2_0 = spectral_norm(c2_0)\n",
    "\n",
    "        c2_1 = nn.Conv2d(ch // 2, ch // 1, 4, 2, 1)\n",
    "        self.c2_1 = spectral_norm(c2_1)\n",
    "\n",
    "        c3_0 = nn.Conv2d(ch // 1, ch // 1, 3, 1, 1)\n",
    "        self.c3_0 = spectral_norm(c3_0)\n",
    "\n",
    "        l4 = nn.Linear(bw * bw * ch, output_dim)\n",
    "        self.l4 = spectral_norm(l4)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    @property\n",
    "    def penult_layer(self):\n",
    "        return self.c3_0\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.lrelu(self.c0_0(x))\n",
    "        h = self.lrelu(self.c0_1(h))\n",
    "        h = self.lrelu(self.c1_0(h))\n",
    "        h = self.lrelu(self.c1_1(h))\n",
    "        h = self.lrelu(self.c2_0(h))\n",
    "        h = self.lrelu(self.c2_1(h))\n",
    "        h = self.lrelu(self.c3_0(h))\n",
    "        h = h.view(x.size(0), -1)\n",
    "        return self.l4(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANWrapper:\n",
    "    def __init__(self, config: DotConfig, device: torch.device, load_weights=True):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "\n",
    "        self.gen = ModelRegistry.create(\n",
    "            config.generator.name, **config.generator.params\n",
    "        ).to(device)\n",
    "        self.dis = ModelRegistry.create(\n",
    "            config.discriminator.name, **config.discriminator.params\n",
    "        ).to(device)\n",
    "\n",
    "        if load_weights:\n",
    "            self.load_weights()\n",
    "\n",
    "        if config.dp:\n",
    "            self.dis.transform = self.dis.module.transform\n",
    "            self.dis.output_layer = self.dis.module.output_layer\n",
    "            self.gen.inverse_transform = self.gen.module.inverse_transform\n",
    "            self.gen.z_dim = self.gen.module.z_dim\n",
    "            self.gen.sample_label = self.gen.module.sample_label\n",
    "\n",
    "        self.eval()\n",
    "        self.define_prior()\n",
    "        self.label = None\n",
    "\n",
    "    def load_weights(self):\n",
    "        gen_path = Path(ROOT_DIR, self.config.generator.ckpt_path)\n",
    "        state_dict = torch.load(\n",
    "            gen_path, map_location=self.device\n",
    "        )\n",
    "        self.gen.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        dis_path = Path(ROOT_DIR, self.config.discriminator.ckpt_path)\n",
    "        state_dict = torch.load(\n",
    "            dis_path,\n",
    "            map_location=self.device,\n",
    "        )\n",
    "        self.dis.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def eval(self):\n",
    "        for param in self.gen.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.dis.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.gen.eval()\n",
    "        self.dis.eval()\n",
    "\n",
    "    def get_latent_code_dim(self):\n",
    "        return self.gen.z_dim\n",
    "\n",
    "    def define_prior(self):\n",
    "        if self.config.prior == \"normal\":\n",
    "            prior = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "                torch.zeros(self.gen.z_dim).to(self.device),\n",
    "                torch.eye(self.gen.z_dim).to(self.device),\n",
    "            )\n",
    "            prior.project = lambda z: z\n",
    "        elif self.config.prior == \"uniform\":\n",
    "            prior = torch.distributions.uniform.Uniform(\n",
    "                -torch.ones(self.gen.z_dim).to(self.device),\n",
    "                torch.ones(self.gen.z_dim).to(self.device),\n",
    "            )\n",
    "            prior.project = lambda z: torch.clip(z, -1 + 1e-9, 1 - 1e-9)\n",
    "            prior.log_prob = lambda z: torch.zeros(z.shape[0], device=z.device)\n",
    "        else:\n",
    "            raise KeyError\n",
    "        self.gen.prior = prior\n",
    "\n",
    "    @property\n",
    "    def transform(self):\n",
    "        return self.dis.transform\n",
    "\n",
    "    @property\n",
    "    def inverse_transform(self):\n",
    "        return self.gen.inverse_transform\n",
    "\n",
    "    @property\n",
    "    def prior(self):\n",
    "        return self.gen.prior\n",
    "\n",
    "    def set_label(self, label):\n",
    "        if self.config.dp:\n",
    "            self.gen.label = label if self.gen.cond else None\n",
    "            self.dis.label = label if self.dis.cond else None\n",
    "            self.label = label if self.dis.cond else None\n",
    "        else:\n",
    "            self.gen.label = label\n",
    "            self.dis.label = label\n",
    "            self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = edict()\n",
    "\n",
    "configs = ['configs/gan_sampling_cifar.yml', 'configs/gan_configs/sngan-ns.yml']\n",
    "configs = [Path(ROOT_DIR, x).as_posix() for x in configs]\n",
    "args.configs = configs\n",
    "\n",
    "params = yaml.round_trip_load(Path(args.configs[0]).open(\"r\"))\n",
    "\n",
    "proc = subprocess.Popen(\"/bin/bash\", stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "out, err = proc.communicate(\n",
    "    (\n",
    "        \" \".join(\n",
    "            [\n",
    "                \"echo\",\n",
    "                '\"' + str(yaml.round_trip_dump(params)) + '\"',\n",
    "                \"|\",\n",
    "                \"cat - \",\n",
    "                *args.configs[1:],\n",
    "            ]\n",
    "        )\n",
    "    ).encode(\"utf-8\")\n",
    ")\n",
    "config = yaml.round_trip_load(out.decode(\"utf-8\"))\n",
    "config = DotConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0)\n",
    "gan = GANWrapper(config.gan_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "# from torch.distributions import Distribution as torchDist\n",
    "\n",
    "\n",
    "class DiscriminatorTarget:\n",
    "    def __init__(\n",
    "        self,\n",
    "        gan,\n",
    "    ):\n",
    "        self.gan = gan\n",
    "        self.proposal = gan.prior\n",
    "\n",
    "    @staticmethod\n",
    "    def latent_target(\n",
    "        z: torch.FloatTensor,\n",
    "        gan,\n",
    "        proposal,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        dgz = gan.dis(gan.gen(z)).squeeze()\n",
    "        logp_z = proposal.log_prob(z)\n",
    "        if dgz.shape != logp_z.shape:\n",
    "            raise Exception\n",
    "        log_prob = (logp_z + dgz) / 1.0\n",
    "\n",
    "        return (log_prob, logp_z, dgz)\n",
    "\n",
    "    def __call__(self, z: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        logp = self.latent_target(z, self.gan, self.proposal)[0]\n",
    "        return logp\n",
    "\n",
    "    def project(self, z):\n",
    "        return self.proposal.project(z)\n",
    "\n",
    "\n",
    "def grad_log_prob(\n",
    "    point: torch.FloatTensor,\n",
    "    log_dens,\n",
    "    # x: Optional[Any] = None,\n",
    ") -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "    point = point.detach().requires_grad_()\n",
    "    log_prob = log_dens(point)\n",
    "    grad = torch.autograd.grad(log_prob.sum(), point)[0]\n",
    "    return log_prob, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(\n",
    "    z: torch.FloatTensor,\n",
    "    target,\n",
    "    proposal,\n",
    "    step_size: float,\n",
    "    n_steps: int = 1,\n",
    "    project: Optional[Callable] = None,\n",
    ") -> List[torch.FloatTensor]:\n",
    "    zs = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        _, grad = grad_log_prob(z, target)\n",
    "        z = z + step_size * grad\n",
    "        z = project(z)\n",
    "        z = z.data\n",
    "        z.requires_grad_(True)\n",
    "        zs.append(z.data)\n",
    "\n",
    "    return zs\n",
    "\n",
    "\n",
    "def ula(\n",
    "    z: torch.FloatTensor,\n",
    "    target,\n",
    "    proposal,\n",
    "    step_size: float,\n",
    "    n_steps: int = 1,\n",
    "    project: Optional[Callable] = None,\n",
    ") -> List[torch.FloatTensor]:\n",
    "    zs = []\n",
    "    device = z.device\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        _, grad = grad_log_prob(z, target)\n",
    "        noise = torch.randn_like(z, dtype=torch.float).to(device)\n",
    "        noise_scale = (2.0 * step_size) ** 0.5\n",
    "        z = z + step_size * grad + noise_scale * noise\n",
    "        z = project(z)\n",
    "        z = z.data\n",
    "        z.requires_grad_(True)\n",
    "        zs.append(z.data)\n",
    "\n",
    "    return zs\n",
    "\n",
    "\n",
    "def isir(\n",
    "    z: torch.FloatTensor,\n",
    "    target,\n",
    "    proposal,\n",
    "    step_size: float,\n",
    "    N: int,\n",
    "    n_steps: int = 1,\n",
    "    project: Optional[Callable] = None,\n",
    ") -> Tuple[List[torch.FloatTensor], float]: \n",
    "    zs = []\n",
    "    batch_size, z_dim = z.shape[:2]\n",
    "    acceptance = torch.zeros(batch_size)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        X = proposal.sample((batch_size, N))\n",
    "        X[np.arange(batch_size), [0] * batch_size, :] = z\n",
    "        X_flat = X.view(-1, z_dim)\n",
    "        log_weight = target(X_flat) - proposal.log_prob(X_flat)\n",
    "        log_weight = log_weight.view(batch_size, N)\n",
    "        max_logs = torch.max(log_weight, dim=1)[0][:, None]\n",
    "        log_weight = log_weight - max_logs\n",
    "        weight = torch.exp(log_weight)\n",
    "        sum_weight = torch.sum(weight, dim=1)\n",
    "        weight = weight / sum_weight[:, None]\n",
    "        weight[weight != weight] = 0.0\n",
    "        weight[weight.sum(1) == 0.0] = 1.0\n",
    "\n",
    "        indices = torch.multinomial(weight, 1).squeeze()\n",
    "        mask = (indices == 0).float()\n",
    "        acceptance += mask.mean().item() / n_steps\n",
    "        z = X[np.arange(batch_size), indices.tolist(), :]\n",
    "\n",
    "        z = project(z)\n",
    "        z = z.data\n",
    "        z.requires_grad_(True)\n",
    "        zs.append(z.data)\n",
    "\n",
    "    return zs, acceptance\n",
    "\n",
    "\n",
    "def heuristics_step_size(\n",
    "    mean_acceptance: float,\n",
    "    target_acceptance: float,\n",
    "    step_size: float,\n",
    "    factor: float = 1.05,\n",
    "    tol: float = 0.03,\n",
    "):\n",
    "    if mean_acceptance - target_acceptance > tol:\n",
    "        return step_size * factor\n",
    "    if target_acceptance - mean_acceptance > tol:\n",
    "        return step_size / factor\n",
    "    return step_size\n",
    "\n",
    "\n",
    "def mala(\n",
    "    z: torch.FloatTensor,\n",
    "    target,\n",
    "    proposal,\n",
    "    step_size: float,\n",
    "    n_steps: int = 1,\n",
    "    project: Optional[Callable] = None,\n",
    "    adapt_stepsize: bool = False,\n",
    ") -> Tuple[List[torch.FloatTensor], float]:\n",
    "    zs = []\n",
    "    batch_size, z_dim = z.shape[0], z.shape[1]\n",
    "\n",
    "    white_noise = torch.distributions.MultivariateNormal(\n",
    "        torch.zeros(z_dim).to(z.device),\n",
    "        torch.eye(z_dim).to(z.device),\n",
    "    )\n",
    "    noise_scale = (2.0 * step_size) ** 0.5\n",
    "    uniform = torch.distributions.Uniform(low=0.0, high=1.0)\n",
    "    acceptance = torch.zeros(batch_size).to(z.device)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        noise = noise_scale * white_noise.sample((batch_size,))\n",
    "        log_prob, grad = grad_log_prob(z, target)\n",
    "        new_z = z + step_size * grad + noise\n",
    "        new_z = new_z.data\n",
    "        new_z.requires_grad_(True)\n",
    "\n",
    "        log_prob_new, grad_new = grad_log_prob(new_z, target)\n",
    "\n",
    "        propose_vec_1 = z - new_z - step_size * grad_new\n",
    "        propose_vec_2 = new_z - z - step_size * grad\n",
    "        propose_part = white_noise.log_prob(propose_vec_1 / noise_scale)\n",
    "        propose_part_new = white_noise.log_prob(propose_vec_2 / noise_scale)\n",
    "        \n",
    "        log_accept_prob = propose_part - propose_part_new + log_prob_new - log_prob\n",
    "\n",
    "        generate_uniform_var = uniform.sample((batch_size,)).to(z.device)\n",
    "        log_generate_uniform_var = torch.log(generate_uniform_var)\n",
    "        mask = log_generate_uniform_var < log_accept_prob\n",
    "\n",
    "        if adapt_stepsize:\n",
    "            mean_acceptance = mask.float().mean()\n",
    "            step_size = heuristics_step_size(\n",
    "                mean_acceptance,\n",
    "                target_acceptance=0.5,\n",
    "                step_size=step_size,\n",
    "            )\n",
    "            noise_scale = (2.0 * step_size) ** 0.5\n",
    "            #print(step_size)\n",
    "\n",
    "        acceptance += mask\n",
    "        with torch.no_grad():\n",
    "            z[mask] = new_z[mask]\n",
    "            z = project(z)\n",
    "            z = z.data\n",
    "            z.requires_grad_(True)\n",
    "            zs.append(z.clone().detach())\n",
    "\n",
    "    return zs, acceptance, step_size\n",
    "\n",
    "\n",
    "def ex2mcmc(\n",
    "    z: torch.FloatTensor,\n",
    "    target,\n",
    "    proposal,\n",
    "    step_size: float,\n",
    "    N: int,\n",
    "    n_steps: int = 1,\n",
    "    project: Optional[Callable] = None,\n",
    "    adapt_stepsize: bool = False,\n",
    ") -> Tuple[List[torch.FloatTensor], float]:\n",
    "    zs = []\n",
    "    for _ in range(n_steps):\n",
    "        z = isir(z, target, proposal, step_size, N, 1, project)[0][-1]\n",
    "        inter_zs, _, step_size = mala(z, target, proposal, step_size, 1, project, adapt_stepsize=adapt_stepsize)\n",
    "        z = inter_zs[-1]\n",
    "        z = z.data\n",
    "        z.requires_grad_(True)\n",
    "        zs.append(z.clone().detach())\n",
    "\n",
    "    return zs, step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = DiscriminatorTarget(gan)\n",
    "\n",
    "z_0 = gan.prior.sample((config.batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.n_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = grad_descent(z_0, target, gan.prior, config.step_size, config.n_steps, gan.prior.project)\n",
    "zs = zs[::config.every]\n",
    "energy = [-target(z).mean().item() for z in zs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7effac862070>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"lines\", linewidth=3)\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "plt.plot(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "178bca306e324e31c3a3b5ac5e68cbefb30375f47c6566422ac974db62a7a08a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('soul4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
